{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8d5c070",
   "metadata": {},
   "source": [
    "Aim - Create a RAG based LLM to fetch accurate information\n",
    "Steps - classify query, get context based on the question, generate answer, evaluate the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af2f60ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ranishreedey/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import TypedDict, List, Annotated\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aed34a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = <your api_key>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22c95a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    query : str\n",
    "    query_type : str\n",
    "    context : str\n",
    "    answer : str\n",
    "    score : str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995697b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_query(state : State):\n",
    "    \"\"\"Classifies a user query as a generic query or a specific topic related.\n",
    "    In this case the Transformer Model\"\"\"\n",
    "    \n",
    "    llm = ChatMistralAI(api_key=api_key, model = \"mistral-large-latest\")\n",
    "    system_prompt = \"\"\"You are an expert in classifying a given query as - generic or transformer-related.\n",
    "                        If the query is anything about the Transformer Model, then classify it as 'transformer-related' else 'generic'.\n",
    "                    \"\"\"\n",
    "\n",
    "    query_type = llm.invoke(input = [(\"system\",system_prompt),\n",
    "                                   (\"human\",state[\"query\"])]).content\n",
    "    \n",
    "    return {\"query_type\" : query_type, \"query\" : state[\"query\"], \"context\": \"\", \"answer\":\"\", \"score\" : \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f640f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(state : State):\n",
    "    \"\"\" Gets the Context from the internal document using RAG\"\"\"\n",
    "    api_key = 'Eajkd7toYyYCEoU1LQiNFcPTvyK3ONep'\n",
    "    doc_loader = PyPDFLoader(\"transformers.pdf\")\n",
    "    doc = doc_loader.load()\n",
    "\n",
    "    doc_splitter = RecursiveCharacterTextSplitter()\n",
    "    doc_chunk = doc_splitter.split_documents(doc)\n",
    "\n",
    "    embedding = MistralAIEmbeddings(api_key=api_key)\n",
    "    vector_store = FAISS.from_documents(doc_chunk, embedding)\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\":1})\n",
    "    context = retriever.invoke(input=state[\"query\"])\n",
    "\n",
    "    return { \"query_type\" : state[\"query_type\"], \"query\" : state[\"query\"], \"context\": context, \"answer\":\"\", \"score\" : \"\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c56f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state : State):\n",
    "    \"\"\"Generate the answer based on the given input and context\"\"\"\n",
    "    prompt = f\"\"\"With the given context answer the query.\n",
    "                    ALWAYS append your answer with 'USING RAG'\n",
    "\n",
    "                    <context>\n",
    "                    {state[\"context\"]}\n",
    "                    </context>\n",
    "                \n",
    "                    query : {state[\"query\"]}\"\"\"\n",
    "    \n",
    "    llm = ChatMistralAI(api_key=api_key, model = \"mistral-large-latest\", max_tokens = 100)\n",
    "    response = llm.invoke(input = [(\"system\",prompt),\n",
    "                                    (\"human\",state[\"query\"])]).content\n",
    "    return {\"query_type\" : state[\"query_type\"], \"query\" : state[\"query\"], \"context\": state[\"context\"], \"answer\":response, \"score\" : \"\"}\n",
    "    # return {\"query\" : state[\"query\"], \"answer\":response}\n",
    "    # return query, answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998812b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(state : State):\n",
    "    \"\"\"Evaluate the answer using another LLM as an evaluator\"\"\"\n",
    "    system_prompt = f\"\"\"With the given query and answer, rate the answer from 1-5. 5 is the highest score.\n",
    "                    {state[\"query\"]}\n",
    "                    {state[\"answer\"]}\n",
    "                    \"\"\"\n",
    "    llm = ChatMistralAI(api_key=api_key, model='mistral-large-latest')\n",
    "    score = llm.invoke(input = [(\"system\",system_prompt),\n",
    "                                   (\"human\",f\"\"\"{state[\"query\"]}\n",
    "                                    {state[\"answer\"]}\"\"\")])\n",
    "    return {\"query_type\" : state[\"query_type\"], \"query\" : state[\"query\"], \"context\": state[\"context\"], \"answer\": state[\"answer\"], \"score\" : score.content}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06de2b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state : State):\n",
    "    \"\"\"Route to the next step. In case of generic query simply answer and avoid fetching context\"\"\"\n",
    "    if 'transformer-related' in state[\"query_type\"].lower():\n",
    "        return \"get_context\"\n",
    "    else:\n",
    "        return END        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff3369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1076bc970>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Initialize the graph to orchestrate the steps\n",
    "graph = StateGraph(State)\n",
    "graph.add_node(\"classify_query\", classify_query)\n",
    "graph.add_node(\"get_context\", get_context)\n",
    "graph.add_node(\"generate\",generate)\n",
    "graph.add_node(\"evaluate\", evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab71fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1076bc970>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Add edges\n",
    "graph.add_edge(START,\"classify_query\")\n",
    "graph.add_conditional_edges(\"classify_query\",router)\n",
    "graph.add_edge(\"get_context\", \"generate\")\n",
    "graph.add_edge(\"generate\",\"evaluate\")\n",
    "graph.add_edge(\"evaluate\", END)\n",
    "\n",
    "graph.set_entry_point(\"classify_query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fe8d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(nodes={'__start__': Node(id='__start__', name='__start__', data=RunnablePassthrough(), metadata=None), 'classify_query': Node(id='classify_query', name='classify_query', data=classify_query(tags=None, recurse=True, explode_args=False, func_accepts_config=False, func_accepts={}), metadata=None), 'get_context': Node(id='get_context', name='get_context', data=get_context(tags=None, recurse=True, explode_args=False, func_accepts_config=False, func_accepts={}), metadata=None), 'generate': Node(id='generate', name='generate', data=generate(tags=None, recurse=True, explode_args=False, func_accepts_config=False, func_accepts={}), metadata=None), 'evaluate': Node(id='evaluate', name='evaluate', data=evaluate(tags=None, recurse=True, explode_args=False, func_accepts_config=False, func_accepts={}), metadata=None), '__end__': Node(id='__end__', name='__end__', data=None, metadata=None)}, edges=[Edge(source='__start__', target='classify_query', data=None, conditional=False), Edge(source='classify_query', target='__end__', data=None, conditional=False)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## compile the graph\n",
    "workflow = graph.compile()\n",
    "workflow.get_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8044b26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ranishreedey/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/ranishreedey/Library/Python/3.9/lib/python/site-packages/langchain_mistralai/embeddings.py:181: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'what is multi head attention in transformers?',\n",
       " 'query_type': 'Based on the query, \"what is multi head attention in transformers?\", this is classified as \\'transformer-related\\' because it specifically asks about a component of the Transformer model architecture.',\n",
       " 'context': [Document(id='940700da-a178-40cc-87a1-dff62b878211', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-09T02:33:09+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-09T02:33:09+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'transformers.pdf', 'total_pages': 10, 'page': 3, 'page_label': '3'}, page_content='the K×D matrices Uq and Uk are the only parameters of this mechanism.10\\nMulti-head self-attention (MHSA). In the self-attention mechanisms de-\\nscribed above, there is one attention matrix which describes the similarity of\\ntwo locations within the sequence. This can act as a bottleneck in the architec-\\nture – it would be useful for pairs of points to be similar in some ‘dimensions’\\nand different in others.11\\nIn order to increase capacity of the first self-attention stage, the transformer\\nblock applies H sets of self-attention in parallel12 (termed H heads) and then\\nlinearly projects the results down to theD×N array required for further pro-\\ncessing. This slight generalisation is calledmulti-head self-attention.\\nY(m) = MHSAθ(X(m−1)) =\\nH∑\\nh=1\\nV(m)\\nh X(m−1)A(m)\\nh , where (4)\\n[A(m)\\nh ]n,n′ =\\nexp\\n((\\nk(m)\\nh,n\\n)⊤\\nq(m)\\nh,n′\\n)\\n∑N\\nn′′=1 exp\\n((\\nk(m)\\nh,n′′\\n)⊤\\nq(m)\\nh,n′\\n) (5)\\nq(m)\\nh,n = U(m)\\nq,h x(m−1)\\nn and k(m)\\nh,n = U(m)\\nk,h x(m−1)\\nn . (6)\\nHere theH matrices V(m)\\nh which areD×Dproject theH self-attention stages\\ndown to the required output dimensionalityD.13\\nThe addition of the matricesV(m)\\nh , and the fact that retaining just the diagonal\\nelements of the attention matrixA(m) will interact the signal instantaneously\\nwith itself, does mean there is some cross-feature processing in multi-head self-\\nattention, as opposed to it containing purely cross-sequence processing. How-\\never, the stage has limited capacity for this type of processing and it is the job\\nof the second stage to address this.\\n3')],\n",
       " 'answer': \"Multi-head self-attention (MHSA) is a mechanism used in transformers to increase the capacity of the self-attention stage. Instead of having a single attention matrix, MHSA applies multiple sets of self-attention in parallel, known as heads. Each head can focus on different aspects or 'dimensions' of the input sequence, allowing the model to capture more complex relationships.\\n\\nThe results from these multiple heads are then linearly projected down to the required\",\n",
       " 'score': 'Based on the given query \"What is multi-head attention in transformers?\" and the provided answer, here\\'s a rating from 1 to 5:\\n\\n**Score: 4.5/5**\\n\\n**Reasoning:**\\n- The answer correctly identifies the term \"Multi-head self-attention (MHSA)\" and its role in transformers.\\n- It explains the key concept of using multiple sets of self-attention in parallel (heads) instead of a single attention matrix.\\n- It highlights that each head can focus on different aspects of the input sequence, allowing the model to capture more complex relationships.\\n- However, the answer does not explicitly mention how the results from these multiple heads are combined. It starts to mention \"linearly projected down\" but does not fully explain the concatenation and linear projection process that follows the multi-head attention mechanism.\\n\\nFor a perfect score (5/5), the answer could have included a bit more detail on the combination of the results from the multiple heads, such as:\\n\\n\"The results from these multiple heads are then concatenated and linearly projected down to the required dimensionality, allowing the model to capture a richer representation of the input sequence.\"\\n\\nOverall, the answer is clear, accurate, and provides a good understanding of multi-head attention in transformers, hence the high score.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## infer/invoke the graph\n",
    "result = workflow.invoke({\"query\":\"what is multi head attention in transformers?\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ccbd42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
